- LLM은 대부분 트랜스포머 아키텍처에서 파생된 AI모델이다
	- 방대한 양의 텍스트 데이터로 학습
	
- LLM과 트랜스포머의 성공은 여러 아이디어의 결합 덕분이다
	-  어텐션, 전이학습, 신경망의 스케일링등 

- 발전 과정
	1. 신경망 언어 모델
	2. Word2vec을 이용한 의미 인코딩
	3. Seq2seq + 어텐션
	4. (현재) 트랜스 포머 + LLM

- ? 트랜스포머 아키텍쳐
	- NLP
- ? 셀프 어텐션 
	- 단어 간의 장거리 종속성(long-range dependendcies)와 문맥관계를 포착할 수 있게 해주는 종류의 어텐션 계산
	- 트랜스포머 아키텍쳐에서 사용

> 이 책의 목적은 트랜스포머를 사용할 수 있도록 하는것

### LLM 정의, 특징
- LLM 두가지 언어 모델링의 조합이 될 수 있는 언어 모델이다
	- 자동 인코딩 작업 : 이전 토큰으로 다음 토큰을 예측하도록 훈련
		- 특정 토큰만 볼 수 있도록 마스크가 전체문장을 가린다
		- 트랜스포머 모델의 디코더 부분에 해당, GPT가 좋은 예이다
	- 자기회귀 작업 : 손상된 입력 내용으로 기존 문장을 재구성하도록 훈련
		- 트랜스포머 모델의 인코더 부분에 해당, BERT가 좋은 예이다
	- Seq2seq 
		- 인코더 : 원시 텍스트를 분리하고 벡터로 변환하는 업무
		- 디코더 : 다음 올 최적의 토큰을 예측
		-  트랜스포머 아키텍처는 Seq2seq 모델이었다

- 그래서 LLM은 세 분류이다
	- 자기회귀 모델(인코더)
		- 택스트를 이해하는 데 뛰어나다
	- 자동 인코딩 모델(디코더)
		- 텍스트를 생성하는 데 뀌어나다
	- 자기회귀 모델 + 자동 인코딩 모델(Seq2seq)
		- 창의적인 텍스트를 생성할 수 있다

### LLM 작동 원리

