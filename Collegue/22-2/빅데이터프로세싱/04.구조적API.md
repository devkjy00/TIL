# 구조적 API 개요
> 학습내용
- 구조적 API의 특징
- 스파크 구조적 데이터 타입 유형
- 구조적 API 실행과정

### 구조적 API의 특징
- 배치(batch) : 요청을 모아서 한번에 처리
- 스트리밍(streaming) : 데이터가 들어올 때마다 지속적으로 처리
- 각 처리 간의 변환도 가능

### 스파크 구조적 데이터 타입 유형
- 유형과 특징
	- DataFrame, DataSet 모두 행(row)과 열(column)을 가지는 분산 테이블의 컬렉션

- 스키마 : 컬럼별로 데이터 타입과 크기를 정의해야 한다

- DataFrame, DataSet 비교
	|구분|DataFrame|DataSet|
	|-|-|-|
	|컬럼 타입 정의|비타입형|타입형|
	|데이터 타입 확인 시점|런타임|컴파일 타임|
	|JVM 필요여부|선택적|필수|
	|동작가능 언어|모든 언어| 자바, 스칼라|

- 스파크 데이터 타입
	```py
	from pyspark.sql.types import *
	b = ByteType()
	```
	- ByteType : int, long , 1byte
	- ShortType : int, long, 2byte 
	- IntegerType, LongType, FloatType, DoubleType, StringType, BooleanType, ArrayType, TimestampType, DateType	

### 구조적 API 실행과정
- 전체적인 흐름
	- 코드작성 -> 논리적 실행계획 -> 검증된 논리적 실행계획(카탈로그로 분석) -> 최적화된 논리적 실행계획 -> 물리적 실행계획(추가적 최적화) -> 실행(RDD 실행)

# 구조적 API 기본연산1
> 학습내용
- DataFrame 생성
- select, selectExpr
- 스파크 데이터 타입으로 변환
- 컬럼 추가, 변경, 제거

### DataFrame 생성
```
>> df = spark.read.format("json").load("dir")
>> df.createOrReplaceTempView("dfTable")
```
	- .load(원시데이터소스) : 데이터를 스파크로 불러 온다
	- createOrReplaceTempView() : DataFreme에서 임시View를 생성
	
- 스키마 만들기
	```
	>>> from pyspark.sql.types import *
	>>> mySchema = StructType([
	... StructField("name1", StringType(), True),
	... StructField("name2", StringType(), True)
	... ])

	>>> df = spark.read.format("json").schema(mySchema).load("dir")
	>>> df.printSchema()
	```
### select와 selectExpr
- SQL을 실행하는 것처럼 동작 가능
```
>>> from pyspark.sql.functions import expr
>>> df.select("name").show(2)
```
	- .select(컬럼) : 컬럼을 선택
	- .show(n) : n개의 데이터 출력
	- expr() : 다른 필드 등의 표현 가능
	- col() : 컬럼에 접근
	- select(SQL문), selectExpr(SQL문) : SQL문 사용가능

### 스파크 데이터 타입으로 변환
- 프로그래밍 언어의 리터럴 값을 스파크가 이해할 수 있는 값으로 변환해야 한다
```
>>> from pyspark.sql.functions import lit
>>> df.select(expr("*"), lit(1).alias("One")).show(1) # One컬럼을 추가하고 값으로 1을 넣는다
```
	- lit(n) : n이라는 값을 전달

### 컬럼 추가, 변경, 제거
```
>>> df.withColumn("numberOne", lit(1)).show(2)
>>> df.withColumn("numberOne", expr("abc == aaa")).show(2) # 표현식 결과값 저장
>>> df.withColumnRenamed("name", "no_name")
>>> df.drop("no_name")
```
	- .withColumn() : 컬럼 추가
	- .withColumnRenamed() : 컬럼명 변경
	- .drop() : 컬럼 삭제


# 구조적 API 기본연산 2
> 학습내용
- 로우 필터링, 정렬, 합치기, 추가
- 무작위 심플과 임의 분할
- repartition과 coalesce

### 함수
- .filter(조건) : 로우 필터링, col() 함수 사용
	-  ```df.filter(col("name")<2).show(2))```
- .where(조건) : SQL문처럼 사용
	-  ```df.where("name < 2").show(2))```
- .select(컬럼) : 컬럼을 선택
- .distict() : 고유한 값만을 가진 DataFrame 반환
- .sample(복원여부, 확률, seed) : 무작위 샘플 반환
	- ```df.sample(0.5, 0.5, 5)```
- .randomSplit(비율1, 비율2, 비율n) : 비율예 다라 임의로 로우를 선정
- from pyspark.sql import Row
	- .parallelize(새로운 DataFrame) : 새로운 열을 병렬처리
		- DataFrame을 변경하려면 다시 생성해야 한다
		```python
		rows = spark.spartContext.parallelize(newRows)
		newDf = spark.createDataFrame(rows, df.schema)	
		```
	
	- .union(새로운 DataFrame) : 조건에 만족하는 새로운 데이터 프레임 생성
		- DataFrame을 추가하려면 기존객체에 통합한다

- from pyspark.sql.functions import desc, asc
	- sort(필드), orderBy(필드) : 필드나 표현식으로 정렬

- repartition(컬럼명) : 컬럼으로 재파티션 수행
	- 자주 필터링하는 컬럼을 기준으로 데이터를 분할해서 클러스터 전반의 물리적인 데이터 구성 제어

- coalesce(n) : n개 파티션으로 병합할 때 사용




