# 구조적 API 개요
> 학습내용
- 구조적 API의 특징
- 스파크 구조적 데이터 타입 유형
- 구조적 API 실행과정

### 구조적 API의 특징
- 배치(batch) : 요청을 모아서 한번에 처리
- 스트리밍(streaming) : 데이터가 들어올 때마다 지속적으로 처리
- 각 처리 간의 변환도 가능

### 스파크 구조적 데이터 타입 유형
- 유형과 특징
	- DataFrame, DataSet 모두 행(row)과 열(column)을 가지는 분산 테이블의 컬렉션

- 스키마 : 컬럼별로 데이터 타입과 크기를 정의해야 한다

- DataFrame, DataSet 비교
	|구분|DataFrame|DataSet|
	|-|-|-|
	|컬럼 타입 정의|비타입형|타입형|
	|데이터 타입 확인 시점|런타임|컴파일 타임|
	|JVM 필요여부|선택적|필수|
	|동작가능 언어|모든 언어| 자바, 스칼라|

- 스파크 데이터 타입
	```py
	from pyspark.sql.types import *
	b = ByteType()
	```
	- ByteType : int, long , 1byte
	- ShortType : int, long, 2byte 
	- IntegerType, LongType, FloatType, DoubleType, StringType, BooleanType, ArrayType, TimestampType, DateType	

### 구조적 API 실행과정
- 전체적인 흐름
	- 코드작성 -> 논리적 실행계획 -> 검증된 논리적 실행계획(카탈로그로 분석) -> 최적화된 논리적 실행계획 -> 물리적 실행계획(추가적 최적화) -> 실행(RDD 실행)

# 구조적 API 기본연산1
> 학습내용
- DataFrame 생성
- select, selectExpr
- 스파크 데이터 타입으로 변환
- 컬럼 추가, 변경, 제거

### DataFrame 생성
```
>> df = spark.read.format("json").load("dir")
>> df.createOrReplaceTempView("dfTable")
```
	- .load(원시데이터소스) : 데이터를 스파크로 불러 온다
	- createOrReplaceTempView() : DataFreme에서 임시View를 생성
	
- 스키마 만들기
	```
	>>> from pyspark.sql.types import *
	>>> mySchema = StructType([
	... StructField("name1", StringType(), True),
	... StructField("name2", StringType(), True)
	... ])

	>>> df = spark.read.format("json").schema(mySchema).load("dir")
	>>> df.printSchema()
	```
### select와 selectExpr
- SQL을 실행하는 것처럼 동작 가능
```
>>> from pyspark.sql.functions import expr
>>> df.select("name").show(2)
```
	- .select(컬럼) : 컬럼을 선택
	- .show(n) : n개의 데이터 출력
	- expr() : 다른 필드 등의 표현 가능
	- col() : 컬럼에 접근
	- select(SQL문), selectExpr(SQL문) : SQL문 사용가능

### 스파크 데이터 타입으로 변환
- 프로그래밍 언어의 리터럴 값을 스파크가 이해할 수 있는 값으로 변환해야 한다
```
>>> from pyspark.sql.functions import lit
>>> df.select(expr("*"), lit(1).alias("One")).show(1) # One컬럼을 추가하고 값으로 1을 넣는다
```
	- lit(n) : n이라는 값을 전달

### 컬럼 추가, 변경, 제거
```
>>> df.withColumn("numberOne", lit(1)).show(2)
>>> df.withColumn("numberOne", expr("abc == aaa")).show(2) # 표현식 결과값 저장
>>> df.withColumnRenamed("name", "no_name")
>>> df.drop("no_name")
```
	- .withColumn() : 컬럼 추가
	- .withColumnRenamed() : 컬럼명 변경
	- .drop() : 컬럼 삭제



