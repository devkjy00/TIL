# 스파크 기본 아키텍처(1)
> 학습내용
- 기본 아키텍처
- 트랜스포메이션, 액션, 스파크 세션
- 스파크 데이터프래임의 파티션과 익스큐터

### 스파크 아키텍처
- 스파크 : 클러스터의 데이터 처리작업을 관리/조율하는 프레임워크
- 클러스터 매니저 : submit을 처리할 자원을 할당해서 결과를 반환시켜준다
	- standalone 클러스터 매니저
	- 하둡 YARN
		- 스파크의 내부적으로 하둡이 동작한다
	- 메소스(mesos)

- 스파크 애플리케이션
	- 드라이버 프로세스 : 스파크 애플리케이션의 수명주기 동안 관련 정보를 모두 유지한다
		- 전반적인 익스큐터 프로세스의 작업과 관련된 분석, 배포, 스케줄링
	- 익스큐터 : 실행자

### 스파크 세션
- 스파크 세션과 언어 API간 관계
	- 파이썬이나 R 코드를 JVM에서 실행할 수 있는 코드로 변환

- 파이썬을 이용한 스파크 세션 생성 예
	- 대화형 모드 : 자동 생성됨
	- 사용자 어플리케이션 
		```py
		# 스파크 세션으로 데이터 프레임 생성
		my_range = spark.range(1000).toDF("number")
		```

### DataFrame
- DataFrame : 테이블의 데이터를 행과 열로 표현
- 파티션 : DF가 저장되는 구역
	- 익스큐터와 다대다 관계로 연결

### 트랜스포메이션
- 트랜스포메이션 : DataFrame을 변경하기위해 변경방법을 스파크에 알리는 명령어
	- 논리적 실행 계획을 세운다
	- 지연연산을 사용, 동작이 실행될 때(액션) 한번에 실행한다

- 작업 유형
	- 1:1(좁은 트랜스포메이션) : 하나의 데이터프레임에서 다른 하나의 데이터프레임으로
		- 파이프라이닝...
	
	- 1:N(넓은 트랜스포메이션) : 하나의 데이터프레임에서 여러 데이터프레임으로
		- 셔플...

- 함수
	- map(func) : 함수의 반환값으로 새로운 분산 데이터셋 반환
		- flatmap(func)
	- filter(func) : 함수의 참으로 선택된 데이터로 분산 데이터셋 반환
	- distinct([작업갯수]) : 유일무이한 작업만 선택해서 인자로 전달, 분산 데이터셋 반환

### 액션
- 액션 : 실제 연산을 수행하는 명령어
- 함수
	- reduce(func) : 데이터셋 원소를 총합요약, func 함수의 인자는 2개
	- take(n) : 첫 n개의 원소로 배열 생성
	- collect() : 배열로 모든 원소를 반환(메모리 용량 사전확인필요)
	- takeOrdered(n, key=func) : 함수의 반환값 기준 or 오름차순으로 n개의 원소를 반환

- 작동과정
	- 액션을 시작하면 스파크 잡(job)이 시작

> 트랜스포메이션(중간연산), 액션(최종연산) -> 자바의 스트림?


# 스파크 기본 아키텍처
> 학습내용
- 데이터프레임 처리과정
- 데이터프레임과 SQL

### 데이터 프레임 처리과정
1. pyspark로 대화형 모드 진입
2. 데이터 읽기
	```py
	>> import os
	>> data2015 = spark.read\
	... .option("inferSchema", "true")\
	... .option("header", "true")\
	... .csv("파일경로")

	>> data2015.take(1) # 데이터 하나 가져오기
	```
	- spark.read... : 로컬의 파일을 읽어 데이터 프레임을 반환
	- take(n) : 데이터프레임의 레코드를 n개 출력

3. 데이터 정렬, 수집
	```py
	data2015.sort("count").explain()
	```
	- .sork(필드) : 특정 필드(컬럼)로 정렬하는 트랜스포메인션으로 실행은 되지 않는다
	- .explain : 실행계획 읽기
	- spark.conf.set("spark.sql.shuffle.partitions", "n") : 스파크의 셔플 출력 파티션 수를 n개로 조정

### 데이터프레임과 SQL
- DF를 테이블로 만들어서 처리
	- createOrReplaceTempView : 데이터 프레임을 테이블이나 뷰로 만들기
	- spark.sql("""sql문""") : 실행계획 읽기
	- .groupBy() : 데이터프레임으로 그룹으로 구분

> 스파크의 실행계획은 비순환 방향 그래프로 설계되어 있고 액션이 호출되면 실행된다


# 스파크 기능
> 학습내용
- 운영용 애플리케이션 실행 
- 머신러닝과 고급분석
- 데이터셋
- 저수준API
- 구조적 스트리밍

### 운영용 애플리케이션 실행
- spark-submit
	- --master : 클러스터 매니저 설정
		- spark://ip:port : 스파크 스탠드얼론 클러스터 사용
		- mesos://ip:port : 아파치 메조스 사용
		- yarn : 하둡 얀 클러스터
		- local : 로컬 모드에서 싱글 코어로 실행
			- [N] : N개 코어로 실행
	- --deploy-mode : 드라이버의 디플로이 모드설정
		- client : 프로그램을 실행하는 노드에서 드라이버 실행
		- cluster : 클러스터 내부의 노드에서 드라이버 실행
	- --class : main함수가 들어 있는 클래스 지정
	- --name : 애플리케이션의 이름 지정, 스파크 웹 UI에 표시
	- --jars : 애플리케이션 실행에 따른 라이
	- --executor-memory : 익스큐터가 사용할 메모리 바이트 용량, 512m. 1g등도 사용 가능
	- --executor-cores : 익스큐터의 코어 개수 설정


### 데이터 셋
- 정적 타입 코드 지원을 위해 고안된 스파크의 구조적 API
	- 대규모 애플리케이션 개발에 유용
	- 파이썬과 R에서 사용할 수 없음

- 비교
	- 데이터 셋 : 타입 안정성, java의 Arraylist등 고정 타입형 컬렉션을 다루는 기능 제공
	- 데이터 프레임 : 다양한 타입의 테이블형 데이터를 보관하는 Row타입의 객체로 구성된 분산 컬렉션

	
- 장점 
	- 필요한 경우만 선택적 사용
	- 처리 결과를 데이터프레임으로 자동 변환해 반환
	- 저수준 API사용 가능, 고수준 API의 SQL을 사용해 빠른 분석 가능
	- Collect메서드나 take메서드를 호출하면 Dataset에서 매개변수로 지정한 타입의 객체 반환

### 구조적 스트리밍
- 안정화된 스트림 처리용 고수준 API
- 연산을 스트리밍 방식으로 실행
- 지연시간을 줄이고 증분 처리 가능

- 실습
	```	py
	>>> dataFrame = spark.read.format("csv")\	
	... .option("header", "true")\
	... .option("inferSchema", "true")\
	... .load("파일경로")

	>>> dataFrame.createOrReplaceTempView("컬럼명")
	>>> schema = dataFrame.schema
	>>> from pyspark.sql.functions import window, column, desc, col
	>>> streamingDataFrame = spark.readStream\
	... .schema(schema)\
	... .option("maxFilesPerTrigger", 1)\
	... .format("csv")\
	... .option("header, "true")
	... .load("파일경로")

	>>> streamingDataFrame.isStreaming # True
	```	
### 머신러닝과 고급분석
- 전처리, 멍잉(munging), 모델학습 및 예측 가능
- 분류, 회귀, 군집화, 딥러닝까지 가능

### 저수준 API
- 스파크RDD를 통해 자바와 파이썬 객체를 다루는 기능
- 스파크의 모든 기능은 RDD기반으로 작성된다
- 고수준 API보다 세밀한 제어 가능
- 메모리에 저장된 원시데이터를 병렬처리 가능
